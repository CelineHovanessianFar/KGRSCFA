# 4: Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations

The paper "Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations" explores how users interact with and perceive the need for counterfactual explanations in recommender systems (RS). Counterfactual explanations are insights provided by RS that explain why certain content was not recommended, offering a contrast with what was recommended.

The study aims to understand when and why users might seek such explanations and how these needs can be integrated into RS to enhance user satisfaction and trust. It delves into various aspects of user interaction with RS, including how users respond to the explanations they encounter in everyday applications like e-commerce and social media. The research identifies triggers that prompt users to seek counterfactual explanations and examines the decision-making utility associated with these explanations.

The findings suggest that users value counterfactual explanations when they help clarify why certain recommendations are made over others, potentially improving the transparency and trustworthiness of RS. The paper also discusses the broader implications of these findings for designing more user-centric RS that cater to diverse needs for explanations, particularly in everyday decision-making contexts.

---

The paper discussed does not detail a specific technical solution or algorithm for generating counterfactual explanations in recommender systems. Instead, it focuses on the empirical understanding of user needs regarding these explanations and the contexts in which users find them valuable. Here’s a breakdown of the primary methodological approaches used to investigate these aspects:

1. **Interview Study**:
    - **Objective**: To explore how everyday users perceive and interact with existing explanations in recommender systems and to identify triggers for seeking counterfactual explanations.
    - **Method**: Semi-structured interviews were conducted with users who regularly interact with online platforms such as e-commerce sites, social media, and multimedia streaming services.
    - **Analysis**: Transcripts were coded and analyzed to extract themes related to users' perception of and interaction with explanations. The study aimed to understand the gaps in current explanation models from a user-centric perspective.
2. **Survey Study**:
    - **Objective**: To quantify the need for counterfactual explanations under different scenarios and to understand how the decision-making utility impacts the desire for these explanations.
    - **Method**: Two scenarios were designed to elicit responses regarding users' needs for counterfactual explanations in different contexts—one involving casual decision-making and another involving decisions with higher stakes (e.g., choosing a restaurant for a special event).
    - **Analysis**: The survey data were analyzed to identify correlations between the perceived utility of decisions and the demand for counterfactual explanations. This helped in understanding how decision context influences explanation needs.

The technical discussion in the paper is more about the conceptual framework for understanding and integrating counterfactual explanations in RS, rather than the implementation of these systems. The paper contributes to the field by:

- Highlighting the importance of user-centered design in explainable AI.
- Demonstrating the potential benefits of counterfactual explanations in making RS more transparent and trustworthy.
- Suggesting that the utility perceived from decisions significantly influences the demand for such explanations.

This research paves the way for future technical developments where the insights gained can be used to design better algorithms that not only provide recommendations but also explain them in ways that align with user expectations and needs.
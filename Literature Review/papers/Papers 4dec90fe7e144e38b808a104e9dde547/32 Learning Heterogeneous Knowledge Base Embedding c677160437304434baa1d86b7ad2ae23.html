<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>32: Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="c6771604-3730-4434-baa1-d86b7ad2ae23" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/attachment_lightgray.svg"/></div><h1 class="page-title">32: Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation</h1><p class="page-description"></p></header><div class="page-body"><p id="eb63cf8a-9248-4066-b658-c66e70e86dba" class="">Features:</p><p id="fa8f902f-7941-4958-9a31-21e4a3861d92" class="">
</p><p id="bafd8218-8916-4f10-851b-471277bac999" class="">Questions:</p><p id="9609ba76-183a-45fd-89f5-4c1184c2f4e4" class="">
</p><hr id="6dbcb456-be64-4fb5-a350-1c6138fe8422"/><p id="c0711ec5-1fb9-4cb9-8664-a0a20538ceae" class="">The authors developed a recommendation system that integrates collaborative filtering with knowledge-base embeddings (KBE). They constructed a knowledge graph representing users, items, and their relationships, and used vector translations to embed these entities and relationships into a unified low-dimensional space. This approach allows the system to generate accurate recommendations and provide personalized explanations by tracing paths in the knowledge graph.</p><hr id="4aa8d59c-b42b-4c01-bc54-4559bde47161"/><h2 id="d0767565-e8a0-4f42-95df-44f668f1a21c" class="">Genearal Summary</h2><p id="41e5359a-f871-4379-a1b3-990fed4f5e6d" class="">This paper proposes an explainable recommendation system that integrates <mark class="highlight-orange_background">structured knowledge bases with collaborative filtering (CF)</mark> to improve recommendation accuracy and provide personalized <mark class="highlight-orange_background">explanations</mark>. The core idea is to leverage <mark class="highlight-orange_background">knowledge-base embeddings (KBE)</mark> to create a unified representation of user behaviors and item properties, allowing the system to generate more accurate recommendations and offer insightful explanations.</p><h3 id="8332dfe2-56f5-4ab7-820f-2093afcf1d2e" class=""><strong>Key Points:</strong></h3><ol type="1" id="4cba8642-ffa8-4514-87f0-3434d75efd08" class="numbered-list" start="1"><li><strong>Model Explanation and Integration</strong>:<ul id="7cb4a97e-b215-496d-9788-b53cc00beb06" class="bulleted-list"><li style="list-style-type:disc">The paper highlights the importance of model-generated explanations in enhancing user experience.</li></ul><ul id="da12d9a4-dc83-4ada-89f9-56ba0182b290" class="bulleted-list"><li style="list-style-type:disc">Traditional recommendation algorithms, particularly CF-based approaches, often rely on unstructured data like textual reviews, images, and various forms of feedback.</li></ul><ul id="b66b2d62-083e-49bc-97c0-9c4b4f524643" class="bulleted-list"><li style="list-style-type:disc">Structured knowledge bases, though previously used in content-based approaches, have been largely overlooked in recent CF-based models despite their potential for providing more personalized recommendations and informed explanations.</li></ul></li></ol><ol type="1" id="c1fed0f4-9bea-4b20-8833-58c68fd8e002" class="numbered-list" start="2"><li><strong>Knowledge-Base Embeddings (KBE)</strong>:<ul id="a89eaeeb-50f4-4193-9bb8-a53e205564d2" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange_background">KBE allows for the integration of large-scale structured and unstructured data, preserving the relationships within the knowledge base.</mark></li></ul><ul id="6d8c9ee5-a686-499e-84d6-b056307f4e82" class="bulleted-list"><li style="list-style-type:disc">This approach helps in learning user and item representations that incorporate explicit knowledge about their relationships.</li></ul></li></ol><ol type="1" id="867bdf50-6101-40d2-a742-56e428fa06f2" class="numbered-list" start="3"><li><strong>Proposed Framework</strong>:<ul id="4c9289d0-0e03-4dac-8dbd-2bfa100fa7c2" class="bulleted-list"><li style="list-style-type:disc">The authors introduce a novel framework that integrates CF with KBE to enhance recommendation performance.</li></ul><ul id="65f7356b-9894-47c8-be63-73b4af03c9cc" class="bulleted-list"><li style="list-style-type:disc">A knowledge graph is constructed, encoding various user behaviors and item properties as relational graphs.</li></ul><ul id="06aec31e-1466-4056-8eee-94c6c407237f" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange_background">A soft matching algorithm is proposed to generate personalized explanations by traversing paths within the knowledge graph.</mark></li></ul></li></ol><ol type="1" id="73678e0c-145b-4868-a847-a551b39b5c46" class="numbered-list" start="4"><li><strong>Experimental Validation</strong>:<ul id="2d0c9935-9ae5-4960-a3f1-07e8d16a4461" class="bulleted-list"><li style="list-style-type:disc">Experiments on real-world e-commerce datasets show that the proposed model outperforms state-of-the-art baselines in recommendation accuracy and explainability.</li></ul><ul id="ae9d454b-8922-4f77-bad2-56ace9a480f6" class="bulleted-list"><li style="list-style-type:disc">The model can effectively integrate heterogeneous data sources and provide meaningful explanations for recommended items.</li></ul></li></ol><h3 id="b3f0494b-747f-4052-baf5-7dfd2dae1aff" class=""><strong>Contributions:</strong></h3><ol type="1" id="1ed7b998-92e9-4816-9e4e-97d9b601544d" class="numbered-list" start="1"><li><strong>Unified Graph Structure</strong>:<ul id="595d256a-0680-475c-9fe7-082d0d41c659" class="bulleted-list"><li style="list-style-type:disc">Integration of heterogeneous multi-type user behaviors and item knowledge into a single graph structure for recommendation.</li></ul></li></ol><ol type="1" id="e5255e01-61f4-4be5-aae0-32ae5e4d77cf" class="numbered-list" start="2"><li><strong>Extended CF Design</strong>:<ul id="e16beb1d-9f7f-41d0-ae6a-65616df08bce" class="bulleted-list"><li style="list-style-type:disc">Extension of traditional CF to operate over the knowledge graph, capturing comprehensive user preferences.</li></ul></li></ol><ol type="1" id="7044abab-3c1f-435c-be9c-07d07cc39aa9" class="numbered-list" start="3"><li><strong>Explanation Generation</strong>:<ul id="1b278142-2b8c-478d-a2fd-0ee618539d41" class="bulleted-list"><li style="list-style-type:disc">A soft matching algorithm for constructing personalized explanations by exploring paths in the graph embedding space.</li></ul></li></ol><h3 id="1531ca17-cd5f-4480-a83c-773daf571603" class=""><strong>Methodology:</strong></h3><ul id="ddf9bf50-e1f0-4ce0-82f9-46990d80a84f" class="bulleted-list"><li style="list-style-type:disc"><strong>Entity and Relation Modeling</strong>:<ul id="83691120-db8c-48a8-922a-4e41dd9f7e0f" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-orange_background">Entities (users, items, words, brands, categories) and relations (purchase, mention, belongs to, produced by, bought together, also bought, also viewed) are embedded into a low-dimensional space.</mark></li></ul><ul id="39c2e182-a697-4407-bf59-1dafc2c7065b" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-orange_background">Relations are modeled as translations in this space, allowing for the calculation of entity similarities.</mark></li></ul></li></ul><ul id="a840699f-5495-4575-aa09-c3921cc64f69" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimization</strong>:<ul id="de43522e-61d2-4093-8f98-0f35402536ef" class="bulleted-list"><li style="list-style-type:circle">The model is optimized by maximizing the likelihood of observed relation triplets, using negative sampling to approximate probabilities.</li></ul></li></ul><ul id="e4563815-86c4-42ba-ad69-b827cd35c7af" class="bulleted-list"><li style="list-style-type:disc"><strong>Explanation Path Construction</strong>:<ul id="dce9f8b3-ff1b-4df1-af5e-48adde428a0b" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-orange_background">Explanation paths are constructed by finding logical inference sequences from users to items within the knowledge graph.</mark></li></ul><ul id="8269a6c7-a728-474a-a18b-ba8e9c393434" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-orange_background">The probability of these paths is computed using soft matching in the embedding space, enabling the generation of natural language explanations.</mark></li></ul></li></ul><h3 id="e6c188d0-b409-41e1-9a1c-017e5a3285c6" class=""><strong>Case Study:</strong></h3><ul id="822f1810-ca26-408c-9d6c-9e03dfb549d0" class="bulleted-list"><li style="list-style-type:disc">A practical example demonstrates how the model generates explanations for a recommended item by linking user behaviors to item properties through the knowledge graph.</li></ul><h3 id="d8a08f2f-ba51-44aa-b3de-4b1f32c0e44d" class=""><strong>Conclusion:</strong></h3><ul id="ab802380-cf32-49d7-92fc-283d1239c78a" class="bulleted-list"><li style="list-style-type:disc">The paper presents a robust approach to incorporating structured knowledge into recommender systems, enhancing both the performance and explainability of recommendations. This approach paves the way for future research into integrating diverse knowledge sources for more sophisticated recommendation systems.</li></ul><hr id="b36c9107-4885-470c-a2e0-4a45d995ae08"/><h2 id="09570800-57e6-4441-b564-2f1e2df0f051" class="">Technical Aspects</h2><h3 id="cefbe07a-5376-40f6-8d2c-1c1677114f7e" class=""><strong>Concept Overview:</strong></h3><p id="25e2e418-ce14-4bb1-839b-bc78652240b1" class="">The authors propose a recommendation system that combines collaborative filtering (CF) with knowledge-base embeddings (KBE) to provide accurate recommendations and personalized explanations. Here&#x27;s how they achieve this:</p><h3 id="a7ba5a24-ac7a-4186-a3a0-0046cf9c848a" class=""><strong>1. Combining Data Sources:</strong></h3><p id="45aa763c-07cf-41da-b753-42c2d325af54" class="">Traditional recommendation systems often use unstructured data like reviews and ratings. However, this solution also incorporates structured data from a knowledge base (KB), which includes clear relationships between entities such as users, items, brands, and categories.</p><h3 id="6e8ea23a-9e33-43cb-b15c-2362e91358f6" class=""><strong>2. Knowledge Graph Creation:</strong></h3><p id="8bda2bce-0df7-4cb7-886f-f2f5d82f880e" class="">A knowledge graph is created where nodes represent entities (e.g., users, items, brands) and edges represent relationships (e.g., user purchased item, item belongs to category). This graph structure helps capture complex interactions and relationships in a unified way.</p><h3 id="d3e13369-7945-4f30-9fbd-1fab19c801ac" class=""><strong>3. Embedding Entities and Relations:</strong></h3><p id="208a10db-b356-4394-83f9-1bac5a616dbf" class="">To make the data usable by machine learning algorithms, the entities and their relationships are embedded into a low-dimensional vector space. This means each entity and relation is represented as a point in this space, preserving the relationships in the knowledge graph.</p><h3 id="027fc97f-31c8-4ba9-8e54-d3f5ff2e84f7" class=""><strong>4. Translating Relations:</strong></h3><p id="0407607a-289e-4c08-851b-de9a724724b1" class="">In the vector space, relationships are modeled as translations. For instance, if a user has bought an item, there is a vector translation from the user vector to the item vector. This helps in maintaining the relational structure while simplifying the computational process.</p><h3 id="9d5cab8c-cf85-446e-ad4f-d7220d57ec95" class=""><strong>5. Generating Recommendations:</strong></h3><p id="7d7b54f4-4118-4224-b08f-6647c8dc70b9" class="">Using these embeddings, the system predicts which items a user is likely to interact with (e.g., purchase) by computing the similarity between the user vector and item vectors. This prediction process leverages the collaborative filtering approach enhanced by the knowledge graph.</p><h3 id="4a41e826-1bae-459f-b39e-bd4b5efd1e50" class=""><strong>6. Creating Explanations:</strong></h3><p id="25b064ac-13f2-4412-bba6-92fe312d68bf" class="">To provide explanations for recommendations, the system looks for paths in the knowledge graph that connect users to recommended items. For example, it might find that a user often buys items from a particular brand and then explain the recommendation of a new item from that brand.</p><h3 id="154ea49e-604e-4ccd-9d17-ecf8de7de0f7" class=""><strong>7. Soft Matching for Flexibility:</strong></h3><p id="20a946c1-8b22-4c31-a02e-f0d58d5ff78f" class="">The system uses a soft matching approach, meaning it doesn&#x27;t require exact matches in the graph but allows for approximate matches. This flexibility helps in finding meaningful connections even if the data is sparse or the relationships are not directly observed.</p><h3 id="8f12f756-ead1-4a8a-a95c-c0d56bc3056d" class=""><strong>Implementation Steps:</strong></h3><ol type="1" id="c53d8f8f-ca0e-4871-a029-e4de1d31e73e" class="numbered-list" start="1"><li><strong>Data Integration</strong>: Combine user behaviors and item attributes into a single knowledge graph.</li></ol><ol type="1" id="493850f6-fac9-4efb-a218-f73c98cf1449" class="numbered-list" start="2"><li><strong>Embedding Training</strong>: Train embeddings for entities and relations so they capture the structure of the knowledge graph.</li></ol><ol type="1" id="f335531f-890a-462d-b5c8-12fa676296c1" class="numbered-list" start="3"><li><strong>Recommendation Generation</strong>: Use these embeddings to predict user-item interactions.</li></ol><ol type="1" id="dcf64e32-329a-4adc-9c32-e871907b4aae" class="numbered-list" start="4"><li><strong>Explanation Path Search</strong>: Find and rank paths in the knowledge graph that explain why an item is recommended to a user.</li></ol><h3 id="f26cca8e-3432-44a3-8f8c-5e03dd76967d" class=""><strong>Practical Example:</strong></h3><p id="6b5aeea6-f758-4241-aa43-4ef8fede458f" class="">If the system recommends a new phone charger to a user, it might explain:</p><ul id="cc9ed03d-7dec-43b3-9dbc-8a22034c94c9" class="bulleted-list"><li style="list-style-type:disc">&quot;You often buy accessories related to phones, and this charger is frequently bought together with items you purchased before.&quot;</li></ul><ul id="96cbde59-6c78-45fc-8803-e83ebde15e24" class="bulleted-list"><li style="list-style-type:disc">&quot;You have shown interest in products from this brand, and this charger is also from the same brand.&quot;</li></ul><hr id="b6200ddc-a8b2-41dd-afd1-1d60052948fb"/><h2 id="db86bb8a-43a5-4434-8e90-76b7fabf1543" class="">Neural Model and Structural Details</h2><p id="7baa8288-e64b-435f-87a9-b05be5650321" class="">
</p><hr id="a43aace0-acc7-4dde-a6fd-1388c85ca4f1"/><h2 id="54fd759b-74e4-48ce-b128-2d0498999eb0" class="">Counterfactual Concept</h2><p id="be68cc1c-4fb1-4891-a2b4-2517fa64b718" class="">
</p><p id="889a8b26-fd74-41f5-8b41-d30daaa37db3" class="">
</p><hr id="b880c8c4-d122-4cea-8595-617d52ed1f08"/><p id="64fe7b4e-f998-46cf-acb8-02650c22f64c" class="">The relation translation in the context of this recommendation system serves several important purposes beyond just computing the similarity between user and item vectors. Here’s why relation translation is necessary:</p><h3 id="4a6598d9-a53a-48ff-b9ea-34b77a8f400b" class=""><strong>1. Capturing Complex Relationships:</strong></h3><ul id="5bf026a4-be65-466f-bece-b1fd09b3dbe4" class="bulleted-list"><li style="list-style-type:disc"><strong>Direct Relations</strong>: <mark class="highlight-orange_background">Simply computing similarities between user and item vectors might miss out on the rich, complex relationships captured in a knowledge graph.</mark> For instance, a user might prefer items from a specific brand or category, or might be influenced by what other similar users have bought.</li></ul><ul id="32c66c33-c1bb-4dba-855f-5df973d7d55f" class="bulleted-list"><li style="list-style-type:disc"><strong>Indirect Relations</strong>: T<mark class="highlight-orange_background">he translation approach allows the system to account for indirect relationships, </mark>like how users who buy one type of product might also be interested in related products (e.g., users who buy cameras might also buy camera accessories).</li></ul><h3 id="08f3824d-649f-44ff-bbce-3cea0da20d3c" class=""><strong>2. Maintaining Relational Structure:</strong></h3><ul id="7f9526e5-ce08-48ae-b0ac-27078562d032" class="bulleted-list"><li style="list-style-type:disc"><strong>Structured Data</strong><mark class="highlight-orange_background">: In a knowledge graph, each relationship (edge) has a specific meaning (e.g., &quot;purchased by&quot;, &quot;belongs to&quot;, &quot;produced by&quot;). Translating these relationships into the vector space ensures that these meanings are preserved.</mark> This helps in understanding not just that two entities are related, but how they are related.</li></ul><ul id="a81357e0-8f65-461a-83f4-98c891c1dc9e" class="bulleted-list"><li style="list-style-type:disc"><strong>Relation-specific Embeddings</strong>: Each type of relationship has its own translation vector, allowing the model to treat different types of interactions (like purchasing vs. viewing) differently.</li></ul><h3 id="2cb33122-b1e4-4ec5-9efc-8b0e34e7919a" class=""><strong>3. Generating Explanations:</strong></h3><ul id="59f70ab0-6952-4099-bb80-08fc0a393df1" class="bulleted-list"><li style="list-style-type:disc"><strong>Path-based Explanations</strong>: <mark class="highlight-orange_background">By translating relationships, the system can trace paths in the knowledge graph to explain why a particular item is recommended</mark>. For example, &quot;You are recommended this phone case because you bought a phone of the same brand.&quot;</li></ul><div><ul id="aac85da4-3545-4e93-91c5-5993d7b53846" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange_background"><strong>Fuzzy Reasoning</strong></mark><mark class="highlight-orange_background">: Translation enables soft matching, which means even if the exact relationship isn’t present, similar relationships can still be used to generate explanations</mark>. This is particularly useful when the data is sparse.</li></ul></div><h3 id="fea78dcb-b2d2-4820-b9a7-21df4f540458" class=""><strong>4. Improved Predictive Power:</strong></h3><ul id="8a8f52f1-c0a0-4ec7-abc1-f473d4f2f549" class="bulleted-list"><li style="list-style-type:disc"><strong>Enhanced Embeddings</strong>: Relation translations help in embedding users and items in a way that inherently captures the underlying knowledge graph’s structure. This leads to more accurate predictions because the embeddings are more informative.</li></ul><ul id="18e21c9e-4441-4081-8dc6-889a5e7c4788" class="bulleted-list"><li style="list-style-type:disc"><strong>Cross-domain Insights</strong>: It allows leveraging relationships across different domains (e.g., purchasing behavior can influence recommendations for related but distinct items like accessories).</li></ul><h3 id="a570b94f-b83e-474d-a134-272a60028b40" class=""><strong>Example to Illustrate:</strong></h3><p id="c8d5861a-a200-4b44-bb55-114f8cc4ff40" class="">Imagine a user who has bought several electronic gadgets. Without relation translation, the system might only consider direct similarities (e.g., users who bought gadgets also bought this gadget). With relation translation, the system can also consider:</p><ul id="9d00a31e-7118-4737-93f6-6559e8f0e69c" class="bulleted-list"><li style="list-style-type:disc"><strong>Category-based Translation</strong>: The user often buys gadgets in the “electronics” category.</li></ul><ul id="94232297-3382-4e0d-8b47-dd615b8191f6" class="bulleted-list"><li style="list-style-type:disc"><strong>Brand-based Translation</strong>: The user prefers gadgets from the brand “TechBrand”.</li></ul><ul id="74c88ae3-e6d3-4813-a6a9-00b0a6208b59" class="bulleted-list"><li style="list-style-type:disc"><strong>Cross-item Relationships</strong>: Gadgets the user bought are frequently bought together with certain accessories.</li></ul><p id="caa32e64-f294-4ca5-a3ef-f6935a9020e6" class="">This richer understanding allows the system to make more nuanced recommendations and provide explanations like:</p><ul id="ff33f0a9-8d92-4575-943e-a4e392cf9450" class="bulleted-list"><li style="list-style-type:disc">&quot;This gadget is recommended because you often buy electronics from TechBrand.&quot;</li></ul><h3 id="886e71bd-1f21-4841-9884-ab5de2503eda" class=""><strong>Conclusion:</strong></h3><p id="762c1ec1-3831-44d2-9423-5a13e2003d88" class="">Relation translation is crucial because it allows the system to leverage the full complexity of the knowledge graph, leading to better recommendations and more meaningful explanations. It’s not just about finding similar items but understanding and utilizing the intricate web of relationships that influence user preferences.</p><hr id="130ca2dd-7ad5-404f-b30d-fe142b9e0cd3"/><p id="556f4d67-8700-4ff5-baf9-675bbe5df4aa" class="">In the context of the paper, the &quot;translation&quot; of an entity (like a product) through a relation (like &quot;described by&quot;) to another entity (like a word) is modeled using knowledge-base embeddings (KBE). Here’s how it works and what happens if a direct connection is not present:</p><h3 id="ed9a83b3-d58f-4992-8c44-2193ffaeea4e" class=""><strong>How Translation Works:</strong></h3><ol type="1" id="a97ea564-0237-486c-82a6-40c2bdf2fd8d" class="numbered-list" start="1"><li><strong>Relation Translation</strong>: Each relation (e.g., &quot;described by&quot;) is represented as a translation vector in the embedding space.</li></ol><ol type="1" id="ff81dcb8-b730-4a08-9f9c-fa2b4ad728df" class="numbered-list" start="2"><li><strong>Entity Embeddings</strong>: Each entity (e.g., a product or a word) is also represented as a vector in this space.</li></ol><ol type="1" id="4ff00de4-ee67-4549-a627-512fa1997240" class="numbered-list" start="3"><li><strong>Vector Arithmetic</strong>: Translating a product through the &quot;described by&quot; relation to a word involves adding the product&#x27;s vector to the &quot;described by&quot; vector to see if it matches the word&#x27;s vector. Mathematically, if <em>p</em> is the product vector, <em>r</em> is the &quot;described by&quot; relation vector, and <em>w</em> is the word vector, the translation is modeled as <em>p</em>+<em>r</em>≈<em>w</em>.<p id="4be850f4-5d49-48ad-907c-f8d8cae34b40" class="">𝑝</p><p id="3fdc971a-88a8-4274-abbe-87a3e5de0cf5" class="">𝑟</p><p id="a9403ea8-10fd-4d37-b143-98c1ff801b24" class="">𝑤</p><p id="85b11368-1956-4ac9-b3c2-706580faccc1" class="">𝑝+𝑟≈𝑤</p></li></ol><h3 id="cebf90d8-4d2c-4527-8423-8e79b5ada6d1" class=""><strong>Handling Missing Connections:</strong></h3><ul id="64587023-9d50-4c7b-827c-d4a68d32d7a7" class="bulleted-list"><li style="list-style-type:disc"><strong>Soft Matching</strong>: If a direct connection is not present in the knowledge graph (KG), the system uses soft matching to find approximate matches. This means it looks for words that are semantically close to the expected result of the translation, even if there isn’t a direct edge in the KG.</li></ul><ul id="0c7d46de-dc2d-4e86-b136-9b4d86f4bb6c" class="bulleted-list"><li style="list-style-type:disc"><strong>Semantic Similarity</strong>: The system leverages the embeddings&#x27; properties where semantically similar entities are close in the vector space. So, if the specific word isn’t directly connected to the product, a semantically similar word might still be used to create a meaningful connection.</li></ul><h3 id="92aff077-abbf-4639-addf-ca643ce9eed4" class=""><strong>Semantic Interpretation:</strong></h3><ul id="b226f688-b55e-4bed-8ab1-091d7c45113d" class="bulleted-list"><li style="list-style-type:disc"><strong>Approximate Matches</strong>: When a word is not directly connected to a product in the KG, finding a close match in the embedding space suggests that the product could be described by a related concept. This maintains the semantic integrity of recommendations and explanations.</li></ul><ul id="109ee2a8-3a22-41ec-9c24-16e51df30fd0" class="bulleted-list"><li style="list-style-type:disc"><strong>Flexibility and Generalization</strong>: This approach allows the model to generalize from the data it has seen, making reasonable inferences even when explicit connections are missing. It ensures that the recommendation system can still function effectively despite data sparsity or incomplete information.</li></ul><h3 id="e67b12fa-ecb4-4bf6-92b5-3d121746f932" class=""><strong>Example:</strong></h3><ul id="62525302-a13e-42c1-b385-1a0ae3cf7c9e" class="bulleted-list"><li style="list-style-type:disc"><strong>Direct Connection</strong>: If a product &quot;Smartphone&quot; is described by the word &quot;Android&quot;, the translation <em>p</em>+<em>r</em>≈<em>w</em> holds directly.<p id="2aef3006-5f05-4138-9c7f-5a5b5aa27736" class="">𝑝+𝑟≈𝑤</p></li></ul><ul id="c675d7bd-cacd-42da-aa7e-81db66f41f7a" class="bulleted-list"><li style="list-style-type:disc"><strong>Missing Direct Connection</strong>: If &quot;Smartphone&quot; is not directly connected to &quot;Android&quot; in the KG, but &quot;Android&quot; is close to &quot;iOS&quot; in the embedding space, the system might still infer that the product is related to mobile operating systems, providing a reasonable explanation based on the nearest available semantic match.</li></ul><p id="ddafb4fd-01d7-4bc5-be8d-596feca7daeb" class="">In summary, if a word is not directly connected to a product in the KG, the system uses soft matching and semantic similarity to find the closest match, ensuring meaningful translations and maintaining the system&#x27;s ability to provide accurate recommendations and explanations.</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
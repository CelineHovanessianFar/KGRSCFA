<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>5: Towards Fair Graph Neural Networks via Graph Counterfactual</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="a5de416e-5b7e-454d-810a-04d61302613f" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/attachment_lightgray.svg"/></div><h1 class="page-title">5: Towards Fair Graph Neural Networks via Graph Counterfactual</h1><p class="page-description"></p></header><div class="page-body"><p id="93f88700-9b2e-44ee-85c3-a3044eee6280" class="">Features:</p><ul id="d83b94b6-4161-4b3e-ada1-aa7e545cbc5a" class="bulleted-list"><li style="list-style-type:disc">GNN</li></ul><hr id="0934acea-4c08-470c-abfa-8f54f83578a7"/><p id="730e9efe-1c50-40e7-a483-54fed37c7c74" class="">The authors developed graph neural network models that use counterfactual methods to ensure fairness, by creating alternate versions of the data (counterfactuals) where sensitive attributes are systematically altered. These models, including frameworks like NIFTY and GEAR, learn to predict outcomes invariant to these attributes by minimizing the discrepancy between original and counterfactual node representations, using techniques such as perturbation and Graph Variational Autoencoders (GraphVAE). The models are trained to disentangle sensitive attributes from other features, ensuring that predictions do not rely on biased or sensitive information.</p><hr id="fa1d270f-c258-43c6-aedc-63ee04568718"/><p id="df9fcaf1-62a6-4702-b4fe-de1a351bc5e0" class="">The paper you provided discusses the development and implementation of fair graph neural networks (GNNs) that utilize a counterfactual approach to address inherent biases in graph-structured data. It highlights the challenge that traditional GNNs, due to their structure and training data, can inadvertently propagate or even amplify existing biases related to sensitive attributes like gender, race, or age. This bias is particularly problematic in applications such as social network analysis, job candidate ranking, and criminal prediction.</p><p id="cab01860-c563-479f-a67c-c93a4f6ba000" class="">To combat this, the authors propose a counterfactual fairness approach in graph modeling. This involves adjusting the data to create counterfactual scenarios that isolate and remove the influence of sensitive attributes on the predictions made by the model. The aim is to ensure that the predictions are the same, regardless of the values of sensitive attributes. This method involves generating counterfactuals—alternate versions of data points where sensitive attributes are modified—which help the model learn to predict outcomes that are independent of these attributes.</p><p id="2791f82a-d7af-4af9-acc1-3f5f41d3dcd4" class="">The paper details the creation of new algorithms and frameworks, such as NIFTY and GEAR, that generate these counterfactuals using methods like perturbation of sensitive attributes and graph variational autoencoders (GraphVAE). It also emphasizes the challenge of achieving realistic counterfactuals that do not distort the underlying data structure or semantics.</p><p id="1e344ec8-7ff2-43de-8524-50ad8724cab8" class="">Overall, the focus of the paper is on developing GNNs that can provide fair and unbiased predictions by understanding and manipulating the causal relationships within graph data, aiming for a balance between accuracy and ethical fairness in model predictions.</p><hr id="0ce94f72-0bb3-42d4-9f82-41f67cf49eeb"/><p id="f6c99e47-714e-4ed6-b578-fc82357e0fc3" class="">Certainly! The technical aspects of the proposed solution for achieving fairness in Graph Neural Networks (GNNs) using a counterfactual approach are quite detailed. Here’s a breakdown of the key components and methodologies:</p><h3 id="026ebd29-bc02-4977-8a8d-245c29078b36" class=""><strong>1. Counterfactual Fairness Concept</strong></h3><p id="abde3fee-dfba-4e3e-9609-0b328c79c899" class="">The core idea is to achieve counterfactual fairness, which means that the outcome of a decision should be the same even if a sensitive attribute (like race or gender) were changed. In other words, the prediction should be independent of any sensitive attributes.</p><h3 id="dfce191d-62e3-4e41-b8dd-c640f6510655" class=""><strong>2. Counterfactual Data Generation</strong></h3><p id="cc5f38e5-1d0c-4a6e-a1e3-2753dcc4a019" class="">The paper introduces methods like NIFTY and GEAR for generating counterfactual data:</p><ul id="1f400375-5468-44ac-8014-4bf0e1ae6859" class="bulleted-list"><li style="list-style-type:disc"><strong>NIFTY</strong>: This method perturbs sensitive attributes to generate counterfactual nodes in a graph. It then trains the model to minimize the difference between the representations of the original and counterfactual nodes, ensuring that the model&#x27;s predictions do not change when the sensitive attribute is altered.</li></ul><ul id="d00e50d8-4db9-476a-aba6-d08a624bd648" class="bulleted-list"><li style="list-style-type:disc"><strong>GEAR</strong>: Utilizes a Graph Variational Autoencoder (GraphVAE) approach. GEAR generates counterfactual representations by first learning to encode and then decode the graph data while altering the sensitive attributes. The model is trained to minimize the discrepancy between the original and counterfactual node representations, aiming to remove the influence of sensitive attributes on the decision-making process.</li></ul><h3 id="1ddc01fb-fa89-4de4-8d28-37103ead9509" class=""><strong>3. Message Passing Mechanism</strong></h3><p id="e761a17e-df71-40e4-82e4-bfc5b78e3eb7" class="">The paper discusses the role of the message passing mechanism in GNNs, where node representations are updated by aggregating information from their neighbors. This mechanism can propagate and even amplify bias because nodes connected in a graph often share similar sensitive attributes. The proposed counterfactual methods aim to break this correlation by ensuring the node representations are invariant to sensitive attributes.</p><h3 id="72370438-9bdd-425a-8e0e-5d8b13bd80d4" class=""><strong>4. Disentangled Representations</strong></h3><p id="78b592f7-9ca9-43d0-b23e-5de7b6a0fea5" class="">The approach involves learning disentangled representations, where the representation of a node is divided into parts:</p><ul id="0fb97e63-0a14-4359-b45b-4b25ebe5b8b3" class="bulleted-list"><li style="list-style-type:disc">One part that is informative of the node&#x27;s label or the target prediction.</li></ul><ul id="2e65332c-a523-4649-ae04-af78446de650" class="bulleted-list"><li style="list-style-type:disc">Another part that is related to the sensitive attribute but should not influence the prediction.</li></ul><p id="23ae96c3-3854-43b6-ab35-c6f4800c0e70" class="">The goal is to ensure that the predictive part of the representation is independent of the sensitive attribute. This disentanglement is crucial for achieving fairness and is enforced through the design of the GNN architecture and loss functions.</p><h3 id="f2a00661-21d1-4958-af2a-87406e3e46d7" class=""><strong>5. Loss Functions and Constraints</strong></h3><p id="f904f055-13e7-4ec2-b713-685bfc8aae0f" class="">The frameworks introduce several loss functions and constraints to guide the learning process:</p><ul id="71de731b-38f2-4a75-bce3-50d1f2fd940c" class="bulleted-list"><li style="list-style-type:disc"><strong>Informativeness Constraint</strong>: Ensures that the content representation (unrelated to sensitive attributes) is informative enough to predict the output accurately.</li></ul><ul id="c2136378-38a0-4e2c-9869-bb176adaed6a" class="bulleted-list"><li style="list-style-type:disc"><strong>Orthogonality Constraint</strong>: Enforces that the content and sensitive attribute representations are orthogonal, meaning they do not share information.</li></ul><ul id="99ba21b3-c44c-4c60-b344-aada1c5c2b8c" class="bulleted-list"><li style="list-style-type:disc"><strong>Reconstruction and Counterfactual Loss</strong>: Minimizes the difference between the actual output and the output predicted using the counterfactual scenario, thereby ensuring that changing the sensitive attribute does not change the prediction.</li></ul><h3 id="3e9d051a-d5c8-4a79-a3d4-9cc75dc6516e" class=""><strong>6. Training and Evaluation</strong></h3><p id="7918ae84-e9a6-43a7-b5ca-583aa1001482" class="">The models are trained on graphs with known biases, and the effectiveness of the counterfactual approach is evaluated by checking if altering the sensitive attributes affects the predictions. This evaluation is crucial to confirm that the models are indeed fair according to the defined metrics.</p><p id="eb70c464-4922-4e81-a914-929a2eee9e24" class="">By integrating these technical elements, the paper proposes a sophisticated method to mitigate bias in GNNs, ensuring that predictions are fair and not influenced by irrelevant sensitive attributes. The techniques are designed to be adaptable to different graph structures and sensitive attributes, making them broadly applicable in various domains where fairness is a concern.</p><hr id="ca662da6-9453-4fef-a7e5-51b2a47c3442"/><p id="a5807046-976c-42bc-989c-1db747332844" class="">The paper you provided describes utilizing Graph Neural Networks (GNNs) with a focus on counterfactual fairness, adapting existing neural network models and proposing novel frameworks to address biases. Here are some more technical details about the neural models used and how data is represented:</p><h3 id="66cf40cb-70c1-40b4-b3f3-d9df8d4a180d" class=""><strong>Neural Models Used</strong></h3><ol type="1" id="6ba61899-85ec-41b0-a76f-b7050900821e" class="numbered-list" start="1"><li><strong>Graph Neural Networks (GNNs)</strong>: The primary neural model used in the paper are GNNs, which are specifically designed to process data represented as graphs. GNNs are effective in learning node representations by aggregating features from their local neighborhood through a series of transformation and aggregation steps.</li></ol><ol type="1" id="075d7994-fce5-4f8f-b680-14ba96d83ec4" class="numbered-list" start="2"><li><strong>Graph Variational Autoencoder (GraphVAE)</strong>: Specifically mentioned in the paper is the use of GraphVAE in the GEAR framework. GraphVAE is a variation of the variational autoencoder that is adapted for graph-structured data. It learns to encode graph data into a latent space and then decode it back, allowing for the generation of counterfactual graphs by altering sensitive attributes during the decoding phase.</li></ol><h3 id="5998ff78-56c3-4593-b24d-7ecf1a700310" class=""><strong>Data Representation</strong></h3><ol type="1" id="cd5c53b0-cfeb-409b-a3c8-345708b11add" class="numbered-list" start="1"><li><strong>Node Features</strong>: Each node in the graph is associated with a feature vector, which contains attributes of the node that can include both sensitive attributes (like gender or race) and non-sensitive attributes.</li></ol><ol type="1" id="539c9455-0f17-4995-88ca-9ed8df688fe2" class="numbered-list" start="2"><li><strong>Graph Structure</strong>: The structure of the graph is represented using an adjacency matrix, which describes the connectivity between nodes. This structure is crucial for the message passing algorithms used in GNNs, where information is propagated between connected nodes.</li></ol><ol type="1" id="f4865095-ba16-4ff6-9d8d-7aac997df511" class="numbered-list" start="3"><li><strong>Disentangled Representations</strong>: The approach involves representing each node with two separate vectors:<ul id="dab26f3e-ec4f-4dad-b864-7b33ae1ff365" class="bulleted-list"><li style="list-style-type:disc"><strong>Content Representation</strong>: Contains information relevant to the task (e.g., predicting user behavior) but is independent of sensitive attributes.</li></ul><ul id="c12d2a91-1a46-4285-a57e-2db4b335c144" class="bulleted-list"><li style="list-style-type:disc"><strong>Sensitive Attribute Representation</strong>: Contains information about the sensitive attribute, which should not influence the prediction outcome.</li></ul></li></ol><h3 id="5587e9cf-f53a-426c-beda-6d6ad6eb5060" class=""><strong>Technical Details of Model Implementation</strong></h3><ol type="1" id="91b66493-3c0d-43d2-a8ce-44fc4859321f" class="numbered-list" start="1"><li><strong>Message Passing Mechanism</strong>: The fundamental operation in GNNs used in the paper involves updating node representations by aggregating features from their neighbors, often using techniques like sum, mean, or max pooling. The specific aggregation functions can vary depending on the GNN variant used (e.g., GCN, GraphSAGE).</li></ol><ol type="1" id="e50e05e3-d06e-44cb-bc9d-aece028d7850" class="numbered-list" start="2"><li><strong>Counterfactual Generation and Training</strong>: The training process involves not just fitting the model to the observed data but also ensuring that the model&#x27;s predictions for the counterfactual scenarios (where sensitive attributes are altered) remain consistent. This involves additional loss functions to penalize discrepancies between factual and counterfactual outcomes.</li></ol><ol type="1" id="cc272441-de50-4122-baf7-c473aa2629c5" class="numbered-list" start="3"><li><strong>Loss Functions</strong>:<ul id="b670f172-d969-4493-a339-d8083d18a039" class="bulleted-list"><li style="list-style-type:disc"><strong>Reconstruction Loss</strong>: In models like GraphVAE, a reconstruction loss ensures that the decoded graph representation closely matches the original graph, both in terms of structure and node features.</li></ul><ul id="3c7b63f6-249d-4fe2-a94a-75897e1a945f" class="bulleted-list"><li style="list-style-type:disc"><strong>Fairness-Oriented Losses</strong>: These include losses designed to minimize the difference in predictions across factual and counterfactual versions of the data, enforcing counterfactual fairness.</li></ul></li></ol><ol type="1" id="670b7535-392f-42ef-97ed-225c25b6c985" class="numbered-list" start="4"><li><strong>Training Procedure</strong>: The models are generally trained using gradient-based optimization techniques, with careful balancing between fitting the data and adhering to fairness constraints.</li></ol><p id="2b049a23-f30f-4453-8603-8435799fc0d2" class="">These elements highlight the complexity and sophistication of the approach, integrating advanced machine learning techniques with ethical considerations to mitigate bias in predictions made by GNNs. This involves a delicate balance between accuracy, fairness, and computational efficiency, tailored to the unique challenges posed by graph-structured data.</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>